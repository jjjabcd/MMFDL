{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import networkx as nx\n",
    "from mmfdl.util.data_gen_modify import make_variable_one\n",
    "from mmfdl.util.utils import formDataset_Single\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "        ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb',\n",
    "         'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr',\n",
    "         'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "        one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        [atom.GetIsAromatic()])\n",
    "\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(f\"input {x} not in allowable set{allowable_set}:\")\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    if mol is None:\n",
    "        return None, None, None\n",
    "    \n",
    "    c_size = mol.GetNumAtoms()\n",
    "    \n",
    "    features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        features.append(feature / sum(feature))\n",
    "\n",
    "    edges = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    edge_index = []\n",
    "    for e1, e2 in g.edges:\n",
    "        edge_index.append([e1, e2])\n",
    "        \n",
    "    return c_size, features, edge_index\n",
    "\n",
    "def infer_max_smiles_len(\n",
    "    smiles_list: List[str],\n",
    "    max_cap: int = 256\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Infer max SMILES length from dataset.\n",
    "\n",
    "    Args:\n",
    "        smiles_list (List[str]): List of SMILES strings.\n",
    "        max_cap (int): Upper cap to avoid extremely long outliers.\n",
    "\n",
    "    Returns:\n",
    "        int: Inferred max length (capped).\n",
    "    \"\"\"\n",
    "    lengths = [len(s) for s in smiles_list if isinstance(s, str) and len(s) > 0]\n",
    "    if len(lengths) == 0:\n",
    "        return 0\n",
    "    inferred = int(max(lengths))\n",
    "    return int(min(inferred, max_cap))\n",
    "\n",
    "\n",
    "def process_csv_to_pt(\n",
    "    csv_path: str,\n",
    "    vocab_path: str,\n",
    "    results_dir: str,\n",
    "    dataset_name: str,\n",
    "    input_col: str = \"SMILES\",\n",
    "    target_col: str = \"Ssel\",\n",
    "    max_smiles_len: Optional[int] = None,\n",
    "    ecfp_bits: int = 2048,\n",
    "    max_cap: int = 256\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate .pt from csv.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to csv file.\n",
    "        vocab_path (str): Path to smiles vocab pkl.\n",
    "        results_dir (str): Output directory.\n",
    "        dataset_name (str): Dataset name for pt.\n",
    "        input_col (str): SMILES column name.\n",
    "        target_col (str): Target column name.\n",
    "        max_smiles_len (Optional[int]): If None, infer from dataset (capped).\n",
    "        ecfp_bits (int): ECFP bit size.\n",
    "        max_cap (int): Upper cap for inferred max length.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: formDataset_Single instance.\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing {csv_path}...\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if input_col not in df.columns:\n",
    "        raise ValueError(f\"Input column '{input_col}' not found. Available: {df.columns.tolist()}\")\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found. Available: {df.columns.tolist()}\")\n",
    "\n",
    "    smiles_list = df[input_col].dropna().astype(str).tolist()\n",
    "    labels = df[target_col].dropna().tolist()\n",
    "\n",
    "    min_len = min(len(smiles_list), len(labels))\n",
    "    smiles_list = smiles_list[:min_len]\n",
    "    labels = labels[:min_len]\n",
    "\n",
    "    if max_smiles_len is None:\n",
    "        max_smiles_len = infer_max_smiles_len(smiles_list, max_cap=max_cap)\n",
    "        if max_smiles_len <= 0:\n",
    "            raise ValueError(\"Failed to infer max_smiles_len (no valid SMILES).\")\n",
    "        print(f\"[INFO] Auto max_smiles_len inferred: {max_smiles_len} (cap={max_cap})\")\n",
    "    else:\n",
    "        print(f\"[INFO] Using provided max_smiles_len: {max_smiles_len}\")\n",
    "\n",
    "    with open(vocab_path, \"rb\") as f:\n",
    "        smilesVoc = pickle.load(f)\n",
    "\n",
    "    encoded_smi_list = []\n",
    "    ecfp_list = []\n",
    "    labels_list = []\n",
    "    smile_graph_dict = {}\n",
    "\n",
    "    valid_count = 0\n",
    "    for idx, (smi, label) in enumerate(zip(smiles_list, labels)):\n",
    "        if pd.isna(smi) or pd.isna(label):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            encoded_smi = make_variable_one(smi, smilesVoc, max_smiles_len)\n",
    "\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            if mol.HasSubstructMatch(Chem.MolFromSmarts(\"[H]\")):\n",
    "                mol = Chem.RemoveHs(mol)\n",
    "\n",
    "            ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=ecfp_bits)\n",
    "            ecfp_array = np.array(ecfp, dtype=np.float32)\n",
    "\n",
    "            c_size, features, edge_index = smile_to_graph(smi)\n",
    "            if edge_index == [] or features is None:\n",
    "                continue\n",
    "\n",
    "            encoded_smi_list.append(encoded_smi)\n",
    "            ecfp_list.append(ecfp_array.tolist())\n",
    "            labels_list.append(float(label))\n",
    "            smile_graph_dict[valid_count] = (c_size, features, edge_index)\n",
    "            valid_count += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing SMILES {idx}: {smi}, error: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"[INFO] valid_count={valid_count} / total={len(smiles_list)}\")\n",
    "\n",
    "    dataset = formDataset_Single(\n",
    "        root=results_dir,\n",
    "        dataset=dataset_name,\n",
    "        encodedSmi=encoded_smi_list,\n",
    "        ecfp=ecfp_list,\n",
    "        y=labels_list,\n",
    "        smile_graph=smile_graph_dict,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'selectivity'\n",
    "# task_name = 'Ki'\n",
    "# vocab_path = f'./data/{dataset_name}/{task_name}/smiles_char_dict.pkl'\n",
    "\n",
    "# with open(vocab_path, 'rb') as f:\n",
    "#     smilesVoc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_fold_num = 1\n",
    "# end_fold_num = 5\n",
    "\n",
    "# for fold_num in range(start_fold_num, end_fold_num + 1):\n",
    "#     input_dir = f'/home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold{fold_num}'\n",
    "#     results_dir = f'./data/{dataset_name}/{task_name}/fold{fold_num}'\n",
    "#     os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "#     # input file path\n",
    "#     train_csv = f'{input_dir}/{dataset_name}_train.csv'\n",
    "#     val_csv = f'{input_dir}/{dataset_name}_val.csv'\n",
    "#     test_csv = f'{input_dir}/{dataset_name}_test.csv'\n",
    "\n",
    "#     # Results file path\n",
    "#     train_pt = f'{results_dir}/{dataset_name}_train.pt'\n",
    "#     val_pt = f'{results_dir}/{dataset_name}_val.pt'\n",
    "#     test_pt = f'{results_dir}/{dataset_name}_test.pt'\n",
    "\n",
    "#     # Generate Train, Validation, Test\n",
    "#     print('\\n' + '=' * 60)\n",
    "#     print(f'Processing Fold {fold_num}')\n",
    "#     print('=' * 60)\n",
    "\n",
    "#     train_dataset = process_csv_to_pt(\n",
    "#         csv_path=train_csv,\n",
    "#         vocab_path=vocab_path,\n",
    "#         results_dir=results_dir,\n",
    "#         dataset_name=f'{dataset_name}_train',\n",
    "#         input_col='SMILES',\n",
    "#         target_col='Ssel',\n",
    "#         ecfp_bits=2048\n",
    "#     )\n",
    "\n",
    "#     val_dataset = process_csv_to_pt(\n",
    "#         csv_path=val_csv,\n",
    "#         vocab_path=vocab_path,\n",
    "#         results_dir=results_dir,\n",
    "#         dataset_name=f'{dataset_name}_val',\n",
    "#         input_col='SMILES',\n",
    "#         target_col='Ssel',\n",
    "#         ecfp_bits=2048\n",
    "#     )\n",
    "\n",
    "#     test_dataset = process_csv_to_pt(\n",
    "#         csv_path=test_csv,\n",
    "#         vocab_path=vocab_path,\n",
    "#         results_dir=results_dir,\n",
    "#         dataset_name=f'{dataset_name}_test',\n",
    "#         input_col='SMILES',\n",
    "#         target_col='Ssel',\n",
    "#         ecfp_bits=2048\n",
    "#     )\n",
    "\n",
    "#     print(f'\\nFold {fold_num} completed!')\n",
    "#     print(f'Results saved to: {results_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_max_smiles_len_from_original_csv(\n",
    "    original_csv_path: str,\n",
    "    smiles_col: str = \"SMILES\",\n",
    "    cap: int = 256,\n",
    "    percentile: Optional[float] = None,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Infer a single max_smiles_len from the original (pre-split) dataset.\n",
    "\n",
    "    Args:\n",
    "        original_csv_path (str): Path to the original dataset CSV (before fold split).\n",
    "        smiles_col (str): Column name for SMILES.\n",
    "        cap (int): Upper cap to avoid extremely long outliers.\n",
    "        percentile (Optional[float]): If provided (e.g., 95.0), use that percentile length\n",
    "            instead of absolute max, then apply cap. If None, use absolute max (then cap).\n",
    "\n",
    "    Returns:\n",
    "        int: max_smiles_len to be used consistently across all folds/splits.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(original_csv_path)\n",
    "    if smiles_col not in df.columns:\n",
    "        raise ValueError(f\"'{smiles_col}' not found in {original_csv_path}. Available: {df.columns.tolist()}\")\n",
    "\n",
    "    smiles_list = df[smiles_col].dropna().astype(str).tolist()\n",
    "    lengths = [len(s) for s in smiles_list if len(s) > 0]\n",
    "\n",
    "    if len(lengths) == 0:\n",
    "        raise ValueError(\"No valid SMILES found to infer max_smiles_len.\")\n",
    "\n",
    "    if percentile is None:\n",
    "        inferred = int(max(lengths))\n",
    "    else:\n",
    "        inferred = int(np.percentile(lengths, percentile))\n",
    "\n",
    "    inferred = int(min(inferred, cap))\n",
    "    return inferred\n",
    "\n",
    "\n",
    "def build_fold_pt_with_global_maxlen(\n",
    "    fold_csv_dir_root: str,\n",
    "    results_dir_root: str,\n",
    "    dataset_name: str,\n",
    "    task_name: str,\n",
    "    vocab_path: str,\n",
    "    global_max_smiles_len: int,\n",
    "    input_col: str = \"SMILES\",\n",
    "    target_col: str = \"Ssel\",\n",
    "    ecfp_bits: int = 2048,\n",
    "    start_fold: int = 1,\n",
    "    end_fold: int = 5,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate train/val/test .pt for each fold using a single global max_smiles_len.\n",
    "\n",
    "    Args:\n",
    "        fold_csv_dir_root (str): Root dir containing fold{n} directories with CSV files.\n",
    "        results_dir_root (str): Root output dir where fold{n} directories will be created.\n",
    "        dataset_name (str): Dataset name prefix.\n",
    "        task_name (str): Task name.\n",
    "        vocab_path (str): Path to smiles vocab pkl.\n",
    "        global_max_smiles_len (int): Global max length inferred from original dataset.\n",
    "        input_col (str): SMILES column name.\n",
    "        target_col (str): Target column name.\n",
    "        ecfp_bits (int): ECFP bit size.\n",
    "        start_fold (int): Start fold index.\n",
    "        end_fold (int): End fold index.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for fold_num in range(start_fold, end_fold + 1):\n",
    "        input_dir = os.path.join(fold_csv_dir_root, f\"fold{fold_num}\")\n",
    "        results_dir = os.path.join(results_dir_root, f\"fold{fold_num}\")\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "        train_csv = os.path.join(input_dir, f\"{dataset_name}_train.csv\")\n",
    "        val_csv = os.path.join(input_dir, f\"{dataset_name}_val.csv\")\n",
    "        test_csv = os.path.join(input_dir, f\"{dataset_name}_test.csv\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Processing Fold {fold_num} | global_max_smiles_len={global_max_smiles_len}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        _ = process_csv_to_pt(\n",
    "            csv_path=train_csv,\n",
    "            vocab_path=vocab_path,\n",
    "            results_dir=results_dir,\n",
    "            dataset_name=f\"{dataset_name}_train\",\n",
    "            input_col=input_col,\n",
    "            target_col=target_col,\n",
    "            max_smiles_len=global_max_smiles_len,\n",
    "            ecfp_bits=ecfp_bits,\n",
    "        )\n",
    "\n",
    "        _ = process_csv_to_pt(\n",
    "            csv_path=val_csv,\n",
    "            vocab_path=vocab_path,\n",
    "            results_dir=results_dir,\n",
    "            dataset_name=f\"{dataset_name}_val\",\n",
    "            input_col=input_col,\n",
    "            target_col=target_col,\n",
    "            max_smiles_len=global_max_smiles_len,\n",
    "            ecfp_bits=ecfp_bits,\n",
    "        )\n",
    "\n",
    "        _ = process_csv_to_pt(\n",
    "            csv_path=test_csv,\n",
    "            vocab_path=vocab_path,\n",
    "            results_dir=results_dir,\n",
    "            dataset_name=f\"{dataset_name}_test\",\n",
    "            input_col=input_col,\n",
    "            target_col=target_col,\n",
    "            max_smiles_len=global_max_smiles_len,\n",
    "            ecfp_bits=ecfp_bits,\n",
    "        )\n",
    "\n",
    "        print(f\"\\nFold {fold_num} completed! Saved to: {results_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Fold 1 | global_max_smiles_len=69\n",
      "============================================================\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold1/selectivity_train.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=1313 / total=1313\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold1/selectivity_val.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=146 / total=146\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold1/selectivity_test.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=365 / total=365\n",
      "\n",
      "Fold 1 completed! Saved to: ./data/selectivity/Ki/fold1\n",
      "\n",
      "============================================================\n",
      "Processing Fold 2 | global_max_smiles_len=69\n",
      "============================================================\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold2/selectivity_train.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=1313 / total=1313\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold2/selectivity_val.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=146 / total=146\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold2/selectivity_test.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=365 / total=365\n",
      "\n",
      "Fold 2 completed! Saved to: ./data/selectivity/Ki/fold2\n",
      "\n",
      "============================================================\n",
      "Processing Fold 3 | global_max_smiles_len=69\n",
      "============================================================\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold3/selectivity_train.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=1313 / total=1313\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold3/selectivity_val.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=146 / total=146\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold3/selectivity_test.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=365 / total=365\n",
      "\n",
      "Fold 3 completed! Saved to: ./data/selectivity/Ki/fold3\n",
      "\n",
      "============================================================\n",
      "Processing Fold 4 | global_max_smiles_len=69\n",
      "============================================================\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold4/selectivity_train.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=1313 / total=1313\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold4/selectivity_val.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=146 / total=146\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold4/selectivity_test.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=365 / total=365\n",
      "\n",
      "Fold 4 completed! Saved to: ./data/selectivity/Ki/fold4\n",
      "\n",
      "============================================================\n",
      "Processing Fold 5 | global_max_smiles_len=69\n",
      "============================================================\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold5/selectivity_train.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=1314 / total=1314\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold5/selectivity_val.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=146 / total=146\n",
      "\n",
      "Processing /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/Ki/fold5/selectivity_test.csv...\n",
      "[INFO] Using provided max_smiles_len: 69\n",
      "[INFO] valid_count=364 / total=364\n",
      "\n",
      "Fold 5 completed! Saved to: ./data/selectivity/Ki/fold5\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"selectivity\"\n",
    "task_name = \"Ki\"\n",
    "\n",
    "# (1) 원본(프리-스플릿) CSV 경로: 사용 중인 파이프라인에 맞게 한 파일을 지정해야 함\n",
    "raw_csv_path = f'/home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/{task_name}_selectivity.csv'\n",
    "\n",
    "# (2) global max_len 산출 (max 대신 95 percentile도 가능)\n",
    "global_max_len = infer_max_smiles_len_from_original_csv(\n",
    "    original_csv_path=raw_csv_path,\n",
    "    smiles_col=\"SMILES\",\n",
    "    cap=256,\n",
    "    percentile=95.0,   # absolute max를 원하면 None\n",
    ")\n",
    "\n",
    "# (3) fold별 CSV가 있는 루트와 결과 루트 지정\n",
    "fold_csv_root = f\"/home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/{task_name}\"\n",
    "results_root = f\"./data/{dataset_name}/{task_name}\"\n",
    "\n",
    "# (4) 생성 실행\n",
    "build_fold_pt_with_global_maxlen(\n",
    "    fold_csv_dir_root=fold_csv_root,\n",
    "    results_dir_root=results_root,\n",
    "    dataset_name=dataset_name,\n",
    "    task_name=task_name,\n",
    "    vocab_path=f\"./data/{dataset_name}/{task_name}/smiles_char_dict.pkl\",\n",
    "    global_max_smiles_len=global_max_len,\n",
    "    input_col=\"SMILES\",\n",
    "    target_col=\"Ssel\",\n",
    "    ecfp_bits=2048,\n",
    "    start_fold=1,\n",
    "    end_fold=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
