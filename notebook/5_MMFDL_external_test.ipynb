{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed7c1e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6abf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import networkx as nx\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric import data as DATA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from mmfdl.util.data_gen_modify import make_variable_one\n",
    "from mmfdl.util.utils_smiecfp import getInput_mask\n",
    "from mmfdl.model.model_combination import comModel\n",
    "from mmfdl.util.utils import formDataset_Single\n",
    "from mmfdl.util.normalization import LabelNormalizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79aa1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "External dataset: /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/davis_selectivity.csv\n",
      "Output directory: results/SGD/selectivity/Ki/external_test\n"
     ]
    }
   ],
   "source": [
    "# 설정 파라미터\n",
    "dataset_name = 'selectivity'\n",
    "task_name = 'Ki'\n",
    "start_fold = 1\n",
    "end_fold = 5\n",
    "ecfp_bits = 2048\n",
    "max_smiles_len = 44\n",
    "batch_size = 256\n",
    "\n",
    "# External dataset 경로\n",
    "external_csv_path = '/home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/davis_selectivity.csv'\n",
    "input_col = 'SMILES'\n",
    "target_col = 'Ssel'\n",
    "\n",
    "# 경로 설정\n",
    "vocab_path = os.path.join('data', dataset_name, task_name, 'smiles_char_dict.pkl')\n",
    "output_dir = os.path.join('results', 'SGD', dataset_name, task_name, 'external_test')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Device 설정\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'External dataset: {external_csv_path}')\n",
    "print(f'Output directory: {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b1e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded: 42 characters\n"
     ]
    }
   ],
   "source": [
    "# 모델 파라미터 설정\n",
    "argsCom = {\n",
    "    'num_features_smi': 44,\n",
    "    'num_features_ecfp': 2048,\n",
    "    'num_features_x': 78,\n",
    "    'dropout': 0.1, \n",
    "    'num_layer': 2,\n",
    "    'num_heads': 2,\n",
    "    'hidden_dim': 256,\n",
    "    'output_dim': 128,\n",
    "    'n_output': 1\n",
    "}\n",
    "\n",
    "# Vocabulary 로드\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    smilesVoc = pickle.load(f)\n",
    "print(f'Vocabulary loaded: {len(smilesVoc)} characters')\n",
    "\n",
    "# Helper functions (MMFDL_geneInput.ipynb에서 가져옴)\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(f\"input {x} not in allowable set{allowable_set}:\")\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "        ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb',\n",
    "         'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr',\n",
    "         'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "        one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        [atom.GetIsAromatic()])\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    if mol is None:\n",
    "        return None, None, []\n",
    "    \n",
    "    c_size = mol.GetNumAtoms()\n",
    "    \n",
    "    features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        features.append(feature / sum(feature))\n",
    "    \n",
    "    edges = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    edge_index = []\n",
    "    for e1, e2 in g.edges:\n",
    "        edge_index.append([e1, e2])\n",
    "        \n",
    "    return c_size, features, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2884acda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing external dataset: /home/rlawlsgurjh/hdd/work/ChEMBLv2/data/selectivity_processed/davis_selectivity.csv...\n",
      "  Found 72 samples\n",
      "  Successfully processed 72 samples\n",
      "External dataset saved to: results/SGD/selectivity/Ki/external_test/external_data\n"
     ]
    }
   ],
   "source": [
    "# External dataset을 .pt 파일로 변환하는 함수\n",
    "def process_external_csv_to_pt(csv_path, vocab_path, results_dir, dataset_name, \n",
    "                               input_col='SMILES', target_col='Ssel', \n",
    "                               max_smiles_len=44, ecfp_bits=2048):\n",
    "    \"\"\"\n",
    "    External CSV 파일을 읽어서 .pt 파일을 생성합니다.\n",
    "    \"\"\"\n",
    "    print(f'\\nProcessing external dataset: {csv_path}...')\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if input_col not in df.columns:\n",
    "        raise ValueError(f\"Input column '{input_col}' not found in CSV. Available columns: {df.columns.tolist()}\")\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in CSV. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    smiles_list = df[input_col].dropna().tolist()\n",
    "    labels = df[target_col].dropna().tolist()\n",
    "    \n",
    "    # 길이가 맞지 않으면 맞춤\n",
    "    min_len = min(len(smiles_list), len(labels))\n",
    "    smiles_list = smiles_list[:min_len]\n",
    "    labels = labels[:min_len]\n",
    "    \n",
    "    print(f'  Found {len(smiles_list)} samples')\n",
    "    \n",
    "    # Vocabulary 로드\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        smilesVoc = pickle.load(f)\n",
    "    \n",
    "    # 데이터 처리\n",
    "    encoded_smi_list = []\n",
    "    ecfp_list = []\n",
    "    labels_list = []\n",
    "    smile_graph_list = []\n",
    "    \n",
    "    valid_count = 0\n",
    "    for idx, (smi, label) in enumerate(zip(smiles_list, labels)):\n",
    "        if pd.isna(smi) or pd.isna(label):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # SMILES 인코딩\n",
    "            encoded_smi = make_variable_one(smi, smilesVoc, max_smiles_len)\n",
    "            \n",
    "            # ECFP 생성\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            if mol.HasSubstructMatch(Chem.MolFromSmarts(\"[H]\")):\n",
    "                mol = Chem.RemoveHs(mol)\n",
    "            ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=ecfp_bits)\n",
    "            ecfp_array = np.array(ecfp, dtype=np.float32)\n",
    "            \n",
    "            # 그래프 생성\n",
    "            c_size, features, edge_index = smile_to_graph(smi)\n",
    "            if edge_index == [] or features is None:\n",
    "                continue\n",
    "            \n",
    "            encoded_smi_list.append(encoded_smi)\n",
    "            ecfp_list.append(ecfp_array.tolist())\n",
    "            labels_list.append(float(label))\n",
    "            smile_graph_list.append((c_size, features, edge_index))\n",
    "            valid_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'  Error processing SMILES {idx}: {smi}, error: {e}')\n",
    "            continue\n",
    "    \n",
    "    print(f'  Successfully processed {valid_count} samples')\n",
    "    \n",
    "    # formDataset_Single을 사용해서 .pt 파일 생성\n",
    "    # smile_graph를 딕셔너리로 변환 (인덱스가 키)\n",
    "    smile_graph_dict = {i: smile_graph_list[i] for i in range(len(smile_graph_list))}\n",
    "    \n",
    "    dataset = formDataset_Single(\n",
    "        root=results_dir,\n",
    "        dataset=dataset_name,\n",
    "        encodedSmi=encoded_smi_list,\n",
    "        ecfp=ecfp_list,\n",
    "        y=labels_list,\n",
    "        smile_graph=smile_graph_dict\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# External dataset을 .pt 파일로 변환\n",
    "external_pt_dir = os.path.join(output_dir, 'external_data')\n",
    "os.makedirs(external_pt_dir, exist_ok=True)\n",
    "external_dataset = process_external_csv_to_pt(\n",
    "    csv_path=external_csv_path,\n",
    "    vocab_path=vocab_path,\n",
    "    results_dir=external_pt_dir,\n",
    "    dataset_name='external_test',\n",
    "    input_col=input_col,\n",
    "    target_col=target_col,\n",
    "    max_smiles_len=max_smiles_len,\n",
    "    ecfp_bits=ecfp_bits\n",
    ")\n",
    "print(f'External dataset saved to: {external_pt_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23abf1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing Fold 1\n",
      "============================================================\n",
      "Train normalizer loaded: mean=0.5742, std=0.4549\n",
      "Loading checkpoint: results/SGD/selectivity/Ki/fold1/best_model.pt\n",
      "Model loaded from epoch 49, val_loss: 0.1753\n",
      "Loaded weights: [0.64522    0.2791582  0.08032811]\n",
      "External normalizer fitted: mean=1.9985, std=1.0922\n",
      "Fold 1 Metrics (train inverse):\n",
      "  RMSE: 1.8102\n",
      "  R2: -1.7473\n",
      "  PCC: -0.1226\n",
      "Fold 1 Metrics (external inverse):\n",
      "  RMSE: 4.3458\n",
      "  R2: -1.7473\n",
      "  PCC: -0.1226\n",
      "Predictions saved: results/SGD/selectivity/Ki/external_test/fold1/predictions.csv\n",
      "Metrics saved: results/SGD/selectivity/Ki/external_test/fold1/metric.csv\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing Fold 2\n",
      "============================================================\n",
      "Train normalizer loaded: mean=0.5751, std=0.4437\n",
      "Loading checkpoint: results/SGD/selectivity/Ki/fold2/best_model.pt\n",
      "Model loaded from epoch 45, val_loss: 0.1213\n",
      "Loaded weights: [0.62579566 0.2780562  0.09741787]\n",
      "External normalizer fitted: mean=1.9985, std=1.0922\n",
      "Fold 2 Metrics (train inverse):\n",
      "  RMSE: 1.7633\n",
      "  R2: -1.6067\n",
      "  PCC: -0.0903\n",
      "Fold 2 Metrics (external inverse):\n",
      "  RMSE: 4.3404\n",
      "  R2: -1.6067\n",
      "  PCC: -0.0903\n",
      "Predictions saved: results/SGD/selectivity/Ki/external_test/fold2/predictions.csv\n",
      "Metrics saved: results/SGD/selectivity/Ki/external_test/fold2/metric.csv\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing Fold 3\n",
      "============================================================\n",
      "Train normalizer loaded: mean=0.5652, std=0.4267\n",
      "Loading checkpoint: results/SGD/selectivity/Ki/fold3/best_model.pt\n",
      "Model loaded from epoch 48, val_loss: 0.2186\n",
      "Loaded weights: [0.62586    0.3119987  0.07042347]\n",
      "External normalizer fitted: mean=1.9985, std=1.0922\n",
      "Fold 3 Metrics (train inverse):\n",
      "  RMSE: 1.6417\n",
      "  R2: -1.2595\n",
      "  PCC: -0.0191\n",
      "Fold 3 Metrics (external inverse):\n",
      "  RMSE: 4.2021\n",
      "  R2: -1.2595\n",
      "  PCC: -0.0191\n",
      "Predictions saved: results/SGD/selectivity/Ki/external_test/fold3/predictions.csv\n",
      "Metrics saved: results/SGD/selectivity/Ki/external_test/fold3/metric.csv\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing Fold 4\n",
      "============================================================\n",
      "Train normalizer loaded: mean=0.5633, std=0.4281\n",
      "Loading checkpoint: results/SGD/selectivity/Ki/fold4/best_model.pt\n",
      "Model loaded from epoch 50, val_loss: 0.2034\n",
      "Loaded weights: [0.63963884 0.28791836 0.07957647]\n",
      "External normalizer fitted: mean=1.9985, std=1.0922\n",
      "Fold 4 Metrics (train inverse):\n",
      "  RMSE: 1.7150\n",
      "  R2: -1.4658\n",
      "  PCC: 0.0160\n",
      "Fold 4 Metrics (external inverse):\n",
      "  RMSE: 4.3749\n",
      "  R2: -1.4658\n",
      "  PCC: 0.0160\n",
      "Predictions saved: results/SGD/selectivity/Ki/external_test/fold4/predictions.csv\n",
      "Metrics saved: results/SGD/selectivity/Ki/external_test/fold4/metric.csv\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing Fold 5\n",
      "============================================================\n",
      "Train normalizer loaded: mean=0.5693, std=0.4421\n",
      "Loading checkpoint: results/SGD/selectivity/Ki/fold5/best_model.pt\n",
      "Model loaded from epoch 50, val_loss: 0.2412\n",
      "Loaded weights: [0.62348276 0.3152611  0.06995709]\n",
      "External normalizer fitted: mean=1.9985, std=1.0922\n",
      "Fold 5 Metrics (train inverse):\n",
      "  RMSE: 1.7730\n",
      "  R2: -1.6354\n",
      "  PCC: -0.0280\n",
      "Fold 5 Metrics (external inverse):\n",
      "  RMSE: 4.3796\n",
      "  R2: -1.6354\n",
      "  PCC: -0.0280\n",
      "Predictions saved: results/SGD/selectivity/Ki/external_test/fold5/predictions.csv\n",
      "Metrics saved: results/SGD/selectivity/Ki/external_test/fold5/metric.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 모든 fold의 metrics를 저장할 리스트\n",
    "all_fold_metrics = []\n",
    "\n",
    "# Fold별로 예측 수행\n",
    "for fold_num in range(start_fold, end_fold + 1):\n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f'Processing Fold {fold_num}')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    # 체크포인트 및 weight 경로\n",
    "    checkpoint_dir = os.path.join('results', 'SGD', dataset_name, task_name, f'fold{fold_num}')\n",
    "    \n",
    "    # 체크포인트에서 best epoch 정보 가져오기\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f'Warning: Checkpoint file not found: {checkpoint_path}')\n",
    "        continue\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    best_epoch = checkpoint['epoch']\n",
    "    \n",
    "    # Weight 파일 경로 (best epoch 기준)\n",
    "    weight_path = os.path.join('results', 'SGD', dataset_name, task_name, f'fold{fold_num}', \n",
    "                               f'{dataset_name}_{task_name}_fold{fold_num}_weight_epoch_{best_epoch}.csv')\n",
    "    \n",
    "    # Train set normalizer 로드\n",
    "    normalizer_train_path = os.path.join(checkpoint_dir, 'normalizer.pkl')\n",
    "    if not os.path.exists(normalizer_train_path):\n",
    "        print(f'Warning: Normalizer file not found: {normalizer_train_path}')\n",
    "        continue\n",
    "    normalizer_train = LabelNormalizer.load(normalizer_train_path)\n",
    "    print(f'Train normalizer loaded: mean={normalizer_train.mean:.4f}, std={normalizer_train.std:.4f}')\n",
    "    \n",
    "    print(f'Loading checkpoint: {checkpoint_path}')\n",
    "    \n",
    "    # 모델 초기화 및 로드\n",
    "    model = comModel(argsCom).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f'Model loaded from epoch {checkpoint[\"epoch\"]}, val_loss: {checkpoint[\"val_loss\"]:.4f}')\n",
    "    \n",
    "    # Weight 로드\n",
    "    weight_df = pd.read_csv(weight_path)\n",
    "    weight_dict = dict(zip(weight_df['Key'], weight_df['Value']))\n",
    "    numpy_weights = np.array([weight_dict[1], weight_dict[2], weight_dict[3]])\n",
    "    print(f'Loaded weights: {numpy_weights}')\n",
    "    \n",
    "    # External test set의 y label 수집 (external normalizer fit용)\n",
    "    external_y_labels = []\n",
    "    for data in external_dataset:\n",
    "        external_y_labels.append(data.y.item())\n",
    "    external_y_labels = np.array(external_y_labels)\n",
    "    \n",
    "    # External test set normalizer 생성 및 fit\n",
    "    normalizer_external = LabelNormalizer(mode='zscore')\n",
    "    normalizer_external.fit(external_y_labels)\n",
    "    print(f'External normalizer fitted: mean={normalizer_external.mean:.4f}, std={normalizer_external.std:.4f}')\n",
    "    \n",
    "    # DataLoader 생성\n",
    "    external_loader = DataLoader(external_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    # 예측 수행\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    pred_data1 = []\n",
    "    pred_data2 = []\n",
    "    pred_data3 = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(external_loader):\n",
    "            encodedSmi = torch.LongTensor(data.smi).to(device)\n",
    "            encodedSmi_mask = torch.LongTensor(getInput_mask(data.smi)).to(device)\n",
    "            ecfp = torch.FloatTensor(data.ep).to(device)\n",
    "            y = data.y.to(device)\n",
    "            x = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            batch = data.batch.to(device)\n",
    "            \n",
    "            # y label 정규화 (train normalizer 사용)\n",
    "            y_norm = torch.FloatTensor(normalizer_train.transform(y.cpu().numpy())).to(device)\n",
    "            \n",
    "            y_pred = model(encodedSmi, encodedSmi_mask, ecfp, x, edge_index, batch)\n",
    "            \n",
    "            all_targets.append(y_norm.cpu().numpy())  # 정규화된 값 저장\n",
    "            pred_data1.append(y_pred[0].cpu().numpy())\n",
    "            pred_data2.append(y_pred[1].cpu().numpy())\n",
    "            pred_data3.append(y_pred[2].cpu().numpy())\n",
    "    \n",
    "    # Weight로 fusion\n",
    "    def flattened_data(data):\n",
    "        fla_data = [item for sublist in data for item in sublist]\n",
    "        merged_data = np.array(fla_data).flatten()\n",
    "        return merged_data\n",
    "    \n",
    "    y_true_norm = flattened_data(all_targets)\n",
    "    y_pred_norm = numpy_weights[0] * flattened_data(pred_data1) + numpy_weights[1] * flattened_data(pred_data2) + numpy_weights[2] * flattened_data(pred_data3)\n",
    "    \n",
    "    # 두 가지 방법으로 inverse_transform\n",
    "    # 1. Train normalizer로 inverse_transform\n",
    "    y_true_train = normalizer_train.inverse_transform(y_true_norm)\n",
    "    y_pred_train = normalizer_train.inverse_transform(y_pred_norm)\n",
    "    \n",
    "    # 2. External normalizer로 inverse_transform\n",
    "    y_true_external = normalizer_external.inverse_transform(y_true_norm)\n",
    "    y_pred_external = normalizer_external.inverse_transform(y_pred_norm)\n",
    "    \n",
    "    # Train normalizer로 inverse_transform한 metrics\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    pcc_train = pearsonr(y_true_train, y_pred_train)[0]\n",
    "    \n",
    "    # External normalizer로 inverse_transform한 metrics\n",
    "    rmse_external = np.sqrt(mean_squared_error(y_true_external, y_pred_external))\n",
    "    r2_external = r2_score(y_true_external, y_pred_external)\n",
    "    pcc_external = pearsonr(y_true_external, y_pred_external)[0]\n",
    "    \n",
    "    # Fold별 metrics 저장 (두 가지 모두)\n",
    "    fold_metrics_train = {\n",
    "        'fold': fold_num,\n",
    "        'inverse_basis': 'train',\n",
    "        'rmse': rmse_train,\n",
    "        'r2': r2_train,\n",
    "        'pcc': pcc_train\n",
    "    }\n",
    "    fold_metrics_external = {\n",
    "        'fold': fold_num,\n",
    "        'inverse_basis': 'external',\n",
    "        'rmse': rmse_external,\n",
    "        'r2': r2_external,\n",
    "        'pcc': pcc_external\n",
    "    }\n",
    "    all_fold_metrics.append(fold_metrics_train)\n",
    "    all_fold_metrics.append(fold_metrics_external)\n",
    "    \n",
    "    print(f'Fold {fold_num} Metrics (train inverse):')\n",
    "    print(f'  RMSE: {rmse_train:.4f}')\n",
    "    print(f'  R2: {r2_train:.4f}')\n",
    "    print(f'  PCC: {pcc_train:.4f}')\n",
    "    print(f'Fold {fold_num} Metrics (external inverse):')\n",
    "    print(f'  RMSE: {rmse_external:.4f}')\n",
    "    print(f'  R2: {r2_external:.4f}')\n",
    "    print(f'  PCC: {pcc_external:.4f}')\n",
    "    \n",
    "    # Fold별 predictions 저장 (train inverse_transform 사용)\n",
    "    fold_output_dir = os.path.join(output_dir, f'fold{fold_num}')\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'y_true': y_true_train,\n",
    "        'y_pred': y_pred_train\n",
    "    })\n",
    "    predictions_path = os.path.join(fold_output_dir, 'predictions.csv')\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "    print(f'Predictions saved: {predictions_path}')\n",
    "    \n",
    "    # Fold별 metric 저장 (두 가지 모두)\n",
    "    metric_df = pd.DataFrame([fold_metrics_train, fold_metrics_external])\n",
    "    metric_path = os.path.join(fold_output_dir, 'metric.csv')\n",
    "    metric_df.to_csv(metric_path, index=False)\n",
    "    print(f'Metrics saved: {metric_path}')\n",
    "    \n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34fa7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary metrics saved: results/SGD/selectivity/Ki/external_test/all_metric.csv\n",
      "\n",
      "Summary Metrics (External Test Set):\n",
      "model_name inverse_basis     rmse  rmse_std        r2   r2_std       pcc  pcc_std\n",
      "     MMFDL         train 1.740642  0.064912 -1.542936 0.187525 -0.048801 0.056291\n",
      "     MMFDL      external 4.328561  0.072749 -1.542936 0.187525 -0.048801 0.056291\n",
      "\n",
      "All folds metrics saved: results/SGD/selectivity/Ki/external_test/all_folds_metrics.csv\n",
      "\n",
      "============================================================\n",
      "External test completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 종합 metrics 계산 및 저장\n",
    "if len(all_fold_metrics) > 0:\n",
    "    # 모든 fold의 metrics를 DataFrame으로 변환\n",
    "    all_metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "    \n",
    "    # inverse_basis별로 그룹화하여 평균과 표준편차 계산\n",
    "    summary_metrics_list = []\n",
    "    for inverse_basis in all_metrics_df['inverse_basis'].unique():\n",
    "        basis_df = all_metrics_df[all_metrics_df['inverse_basis'] == inverse_basis]\n",
    "        summary_metrics = {\n",
    "            'model_name': 'MMFDL',\n",
    "            'inverse_basis': inverse_basis,\n",
    "            'rmse': basis_df['rmse'].mean(),\n",
    "            'rmse_std': basis_df['rmse'].std(), \n",
    "            'r2': basis_df['r2'].mean(),\n",
    "            'r2_std': basis_df['r2'].std(),\n",
    "            'pcc': basis_df['pcc'].mean(),\n",
    "            'pcc_std': basis_df['pcc'].std(),\n",
    "        }\n",
    "        summary_metrics_list.append(pd.DataFrame([summary_metrics]))\n",
    "    \n",
    "    summary_df = pd.concat(summary_metrics_list, ignore_index=True)\n",
    "    \n",
    "    # 종합 metric.csv 저장\n",
    "    summary_path = os.path.join(output_dir, 'all_metric.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f'\\nSummary metrics saved: {summary_path}')\n",
    "    print('\\nSummary Metrics (External Test Set):')\n",
    "    print(summary_df.to_string(index=False))\n",
    "    \n",
    "    # 모든 fold의 상세 metrics도 저장\n",
    "    all_metrics_path = os.path.join(output_dir, 'all_folds_metrics.csv')\n",
    "    all_metrics_df.to_csv(all_metrics_path, index=False)\n",
    "    print(f'\\nAll folds metrics saved: {all_metrics_path}')\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('External test completed!')\n",
    "    print('=' * 60)\n",
    "else:\n",
    "    print('No metrics to save!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
