{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6abf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: /HDD1/hwan1155/work/LT-MMFE/mmfdl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import networkx as nx\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric import data as DATA\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'notebook':\n",
    "    os.chdir('..')\n",
    "\n",
    "print(f\"path: {os.getcwd()}\")\n",
    "\n",
    "from mmfdl.util.data_gen_modify import make_variable_one\n",
    "from mmfdl.util.utils_smiecfp import getInput_mask\n",
    "from mmfdl.model.model_combination import comModel\n",
    "from mmfdl.util.utils import formDataset_Single\n",
    "from mmfdl.util.normalization import LabelNormalizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79aa1094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "External dataset: ./../chembl/data/selectivity_processed/davis_selectivity.csv\n",
      "Output directory: results/SGD/selectivity/WS(alpha=2)/Kd/external_test\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'selectivity'\n",
    "task_name = 'Kd'\n",
    "start_fold = 1\n",
    "end_fold = 5\n",
    "ecfp_bits = 2048\n",
    "max_smiles_len = 44\n",
    "batch_size = 256\n",
    "\n",
    "external_csv_path = './../chembl/data/selectivity_processed/davis_selectivity.csv'\n",
    "input_col = 'SMILES'\n",
    "####################\n",
    "target_cols=[\n",
    "    'S(10uM)', 'S(1uM)', 'S(100nM)',\n",
    "    'PI', 'Ssel', 'WS(alpha=2)', 'WS(alpha=1)',\n",
    "    'RS(k=3)', 'RS(k=2)']\n",
    "\n",
    "target_col = target_cols[5]\n",
    "#########################\n",
    "\n",
    "vocab_path = os.path.join('data', dataset_name, task_name, 'smiles_char_dict.pkl')\n",
    "output_dir = os.path.join('results', 'SGD', dataset_name, target_col, task_name, 'external_test')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'External dataset: {external_csv_path}')\n",
    "print(f'Output directory: {output_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "argsCom = {\n",
    "    'num_features_smi': 44,\n",
    "    'num_features_ecfp': 2048,\n",
    "    'num_features_x': 78,\n",
    "    'dropout': 0.1, \n",
    "    'num_layer': 2,\n",
    "    'num_heads': 2,\n",
    "    'hidden_dim': 256,\n",
    "    'output_dim': 128,\n",
    "    'n_output': 1\n",
    "}\n",
    "\n",
    "# Load Vocabulary\n",
    "with open(vocab_path, 'rb') as f:\n",
    "    smilesVoc = pickle.load(f)\n",
    "print(f'Vocabulary loaded: {len(smilesVoc)} characters')\n",
    "\n",
    "# Helper functions\n",
    "def one_of_k_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(f\"input {x} not in allowable set{allowable_set}:\")\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def one_of_k_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return list(map(lambda s: x == s, allowable_set))\n",
    "\n",
    "def atom_features(atom):\n",
    "    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n",
    "        ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na','Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb',\n",
    "         'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H','Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr',\n",
    "         'Cr', 'Pt', 'Hg', 'Pb', 'Unknown']) +\n",
    "        one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n",
    "        [atom.GetIsAromatic()])\n",
    "\n",
    "def smile_to_graph(smile):\n",
    "    mol = Chem.MolFromSmiles(smile)\n",
    "    if mol is None:\n",
    "        return None, None, []\n",
    "    \n",
    "    c_size = mol.GetNumAtoms()\n",
    "    \n",
    "    features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        feature = atom_features(atom)\n",
    "        features.append(feature / sum(feature))\n",
    "    \n",
    "    edges = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n",
    "    g = nx.Graph(edges).to_directed()\n",
    "    edge_index = []\n",
    "    for e1, e2 in g.edges:\n",
    "        edge_index.append([e1, e2])\n",
    "        \n",
    "    return c_size, features, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2884acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_external_csv_to_pt(csv_path, vocab_path, results_dir, dataset_name, \n",
    "                               input_col='SMILES', target_col='Ssel', \n",
    "                               max_smiles_len=44, ecfp_bits=2048):\n",
    "\n",
    "    print(f'\\nProcessing external dataset: {csv_path}...')\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    if input_col not in df.columns:\n",
    "        raise ValueError(f\"Input column '{input_col}' not found in CSV. Available columns: {df.columns.tolist()}\")\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in CSV. Available columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    smiles_list = df[input_col].dropna().tolist()\n",
    "    labels = df[target_col].dropna().tolist()\n",
    "\n",
    "    min_len = min(len(smiles_list), len(labels))\n",
    "    smiles_list = smiles_list[:min_len]\n",
    "    labels = labels[:min_len]\n",
    "    \n",
    "    print(f'  Found {len(smiles_list)} samples')\n",
    "    \n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        smilesVoc = pickle.load(f)\n",
    "\n",
    "    encoded_smi_list = []\n",
    "    ecfp_list = []\n",
    "    labels_list = []\n",
    "    smile_graph_list = []\n",
    "    \n",
    "    valid_count = 0\n",
    "    for idx, (smi, label) in enumerate(zip(smiles_list, labels)):\n",
    "        if pd.isna(smi) or pd.isna(label):\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # SMILES encoding\n",
    "            encoded_smi = make_variable_one(smi, smilesVoc, max_smiles_len)\n",
    "            \n",
    "            # Generate ECFP\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                continue\n",
    "            if mol.HasSubstructMatch(Chem.MolFromSmarts(\"[H]\")):\n",
    "                mol = Chem.RemoveHs(mol)\n",
    "            ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=ecfp_bits)\n",
    "            ecfp_array = np.array(ecfp, dtype=np.float32)\n",
    "            \n",
    "            # Generate Graph\n",
    "            c_size, features, edge_index = smile_to_graph(smi)\n",
    "            if edge_index == [] or features is None:\n",
    "                continue\n",
    "            \n",
    "            encoded_smi_list.append(encoded_smi)\n",
    "            ecfp_list.append(ecfp_array.tolist())\n",
    "            labels_list.append(float(label))\n",
    "            smile_graph_list.append((c_size, features, edge_index))\n",
    "            valid_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'  Error processing SMILES {idx}: {smi}, error: {e}')\n",
    "            continue\n",
    "    \n",
    "    print(f'  Successfully processed {valid_count} samples')\n",
    "    \n",
    "    smile_graph_dict = {i: smile_graph_list[i] for i in range(len(smile_graph_list))}\n",
    "    \n",
    "    dataset = formDataset_Single(\n",
    "        root=results_dir,\n",
    "        dataset=dataset_name,\n",
    "        encodedSmi=encoded_smi_list,\n",
    "        ecfp=ecfp_list,\n",
    "        y=labels_list,\n",
    "        smile_graph=smile_graph_dict\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "external_pt_dir = os.path.join(output_dir, 'external_data')\n",
    "os.makedirs(external_pt_dir, exist_ok=True)\n",
    "external_dataset = process_external_csv_to_pt(\n",
    "    csv_path=external_csv_path,\n",
    "    vocab_path=vocab_path,\n",
    "    results_dir=external_pt_dir,\n",
    "    dataset_name='external_test',\n",
    "    input_col=input_col,\n",
    "    target_col=target_col,\n",
    "    max_smiles_len=max_smiles_len,\n",
    "    ecfp_bits=ecfp_bits\n",
    ")\n",
    "print(f'External dataset saved to: {external_pt_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23abf1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fold_metrics = []\n",
    "\n",
    "for fold_num in range(start_fold, end_fold + 1):\n",
    "    print('\\n' + '=' * 60)\n",
    "    print(f'Processing Fold {fold_num}')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    checkpoint_dir = os.path.join('results', 'SGD', dataset_name, task_name, target_col, f'fold{fold_num}')\n",
    "    \n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f'Warning: Checkpoint file not found: {checkpoint_path}')\n",
    "        continue\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    best_epoch = checkpoint['epoch']\n",
    "\n",
    "    weight_path = os.path.join('results', 'SGD', dataset_name, task_name, target_col, f'fold{fold_num}', \n",
    "                               f'{dataset_name}_{task_name}_fold{fold_num}_weight_epoch_{best_epoch}.csv')\n",
    "\n",
    "    normalizer_train_path = os.path.join(checkpoint_dir, 'normalizer.pkl')\n",
    "    if not os.path.exists(normalizer_train_path):\n",
    "        print(f'Warning: Normalizer file not found: {normalizer_train_path}')\n",
    "        continue\n",
    "    normalizer_train = LabelNormalizer.load(normalizer_train_path)\n",
    "    print(f'Train normalizer loaded: mean={normalizer_train.mean:.4f}, std={normalizer_train.std:.4f}')\n",
    "    \n",
    "    print(f'Loading checkpoint: {checkpoint_path}')\n",
    "    \n",
    "    # Load Model & Initialize Model\n",
    "    model = comModel(argsCom).to(device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f'Model loaded from epoch {checkpoint[\"epoch\"]}, val_loss: {checkpoint[\"val_loss\"]:.4f}')\n",
    "    \n",
    "    # Load Weight\n",
    "    weight_df = pd.read_csv(weight_path)\n",
    "    weight_dict = dict(zip(weight_df['Key'], weight_df['Value']))\n",
    "    numpy_weights = np.array([weight_dict[1], weight_dict[2], weight_dict[3]])\n",
    "    print(f'Loaded weights: {numpy_weights}')\n",
    "    \n",
    "    external_y_labels = []\n",
    "    for data in external_dataset:\n",
    "        external_y_labels.append(data.y.item())\n",
    "    external_y_labels = np.array(external_y_labels)\n",
    "    \n",
    "    normalizer_external = LabelNormalizer(mode='zscore')\n",
    "    normalizer_external.fit(external_y_labels)\n",
    "    print(f'External normalizer fitted: mean={normalizer_external.mean:.4f}, std={normalizer_external.std:.4f}')\n",
    "\n",
    "    external_loader = DataLoader(external_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    pred_data1 = []\n",
    "    pred_data2 = []\n",
    "    pred_data3 = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(external_loader):\n",
    "            encodedSmi = torch.LongTensor(data.smi).to(device)\n",
    "            encodedSmi_mask = torch.LongTensor(getInput_mask(data.smi)).to(device)\n",
    "            ecfp = torch.FloatTensor(data.ep).to(device)\n",
    "            y = data.y.to(device)\n",
    "            x = data.x.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            batch = data.batch.to(device)\n",
    "\n",
    "            y_norm = torch.FloatTensor(normalizer_train.transform(y.cpu().numpy())).to(device)\n",
    "            \n",
    "            y_pred = model(encodedSmi, encodedSmi_mask, ecfp, x, edge_index, batch)\n",
    "            \n",
    "            all_targets.append(y_norm.cpu().numpy())\n",
    "            pred_data1.append(y_pred[0].cpu().numpy())\n",
    "            pred_data2.append(y_pred[1].cpu().numpy())\n",
    "            pred_data3.append(y_pred[2].cpu().numpy())\n",
    "\n",
    "    def flattened_data(data):\n",
    "        fla_data = [item for sublist in data for item in sublist]\n",
    "        merged_data = np.array(fla_data).flatten()\n",
    "        return merged_data\n",
    "    \n",
    "    y_true_norm = flattened_data(all_targets)\n",
    "    y_pred_norm = numpy_weights[0] * flattened_data(pred_data1) + numpy_weights[1] * flattened_data(pred_data2) + numpy_weights[2] * flattened_data(pred_data3)\n",
    "    \n",
    "    y_true_train = normalizer_train.inverse_transform(y_true_norm)\n",
    "    y_pred_train = normalizer_train.inverse_transform(y_pred_norm)\n",
    "\n",
    "    y_true_external = normalizer_external.inverse_transform(y_true_norm)\n",
    "    y_pred_external = normalizer_external.inverse_transform(y_pred_norm)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_true_train, y_pred_train))\n",
    "    r2_train = r2_score(y_true_train, y_pred_train)\n",
    "    pcc_train = pearsonr(y_true_train, y_pred_train)[0]\n",
    "\n",
    "    rmse_external = np.sqrt(mean_squared_error(y_true_external, y_pred_external))\n",
    "    r2_external = r2_score(y_true_external, y_pred_external)\n",
    "    pcc_external = pearsonr(y_true_external, y_pred_external)[0]\n",
    "    \n",
    "    fold_metrics_train = {\n",
    "        'fold': fold_num,\n",
    "        'inverse_basis': 'train',\n",
    "        'rmse': rmse_train,\n",
    "        'r2': r2_train,\n",
    "        'pcc': pcc_train\n",
    "    }\n",
    "    fold_metrics_external = {\n",
    "        'fold': fold_num,\n",
    "        'inverse_basis': 'external',\n",
    "        'rmse': rmse_external,\n",
    "        'r2': r2_external,\n",
    "        'pcc': pcc_external\n",
    "    }\n",
    "    all_fold_metrics.append(fold_metrics_train)\n",
    "    all_fold_metrics.append(fold_metrics_external)\n",
    "    \n",
    "    print(f'Fold {fold_num} Metrics (train inverse):')\n",
    "    print(f'  RMSE: {rmse_train:.4f}')\n",
    "    print(f'  R2: {r2_train:.4f}')\n",
    "    print(f'  PCC: {pcc_train:.4f}')\n",
    "    print(f'Fold {fold_num} Metrics (external inverse):')\n",
    "    print(f'  RMSE: {rmse_external:.4f}')\n",
    "    print(f'  R2: {r2_external:.4f}')\n",
    "    print(f'  PCC: {pcc_external:.4f}')\n",
    "\n",
    "    fold_output_dir = os.path.join(output_dir, f'fold{fold_num}')\n",
    "    os.makedirs(fold_output_dir, exist_ok=True)\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'y_true': y_true_train,\n",
    "        'y_pred': y_pred_train\n",
    "    })\n",
    "    predictions_path = os.path.join(fold_output_dir, 'predictions.csv')\n",
    "    predictions_df.to_csv(predictions_path, index=False)\n",
    "    print(f'Predictions saved: {predictions_path}')\n",
    "\n",
    "    metric_df = pd.DataFrame([fold_metrics_train, fold_metrics_external])\n",
    "    metric_path = os.path.join(fold_output_dir, 'metric.csv')\n",
    "    metric_df.to_csv(metric_path, index=False)\n",
    "    print(f'Metrics saved: {metric_path}')\n",
    "    \n",
    "    print('=' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fa7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_fold_metrics) > 0:\n",
    "    all_metrics_df = pd.DataFrame(all_fold_metrics)\n",
    "\n",
    "    summary_metrics_list = []\n",
    "    for inverse_basis in all_metrics_df['inverse_basis'].unique():\n",
    "        basis_df = all_metrics_df[all_metrics_df['inverse_basis'] == inverse_basis]\n",
    "        summary_metrics = {\n",
    "            'model_name': 'MMFDL',\n",
    "            'inverse_basis': inverse_basis,\n",
    "            'rmse': basis_df['rmse'].mean(),\n",
    "            'rmse_std': basis_df['rmse'].std(), \n",
    "            'r2': basis_df['r2'].mean(),\n",
    "            'r2_std': basis_df['r2'].std(),\n",
    "            'pcc': basis_df['pcc'].mean(),\n",
    "            'pcc_std': basis_df['pcc'].std(),\n",
    "        }\n",
    "        summary_metrics_list.append(pd.DataFrame([summary_metrics]))\n",
    "    \n",
    "    summary_df = pd.concat(summary_metrics_list, ignore_index=True)\n",
    "\n",
    "    summary_path = os.path.join(output_dir, 'all_metric.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f'\\nSummary metrics saved: {summary_path}')\n",
    "    print('\\nSummary Metrics (External Test Set):')\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    all_metrics_path = os.path.join(output_dir, 'all_folds_metrics.csv')\n",
    "    all_metrics_df.to_csv(all_metrics_path, index=False)\n",
    "    print(f'\\nAll folds metrics saved: {all_metrics_path}')\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('External test completed!')\n",
    "    print('=' * 60)\n",
    "else:\n",
    "    print('No metrics to save!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fe4e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8886fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmfdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
